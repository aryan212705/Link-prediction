# -*- coding: utf-8 -*-
"""Final Year Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14XK4Dt7uD2QqDPl13bZBVnCe5p9ST1jQ
"""

import networkx as nx
import matplotlib.pyplot as plt
import numpy as np
from collections import Counter
import random as rn
import bisect
from multiprocessing import Pool, Process
from functools import partial
import matplotlib.pyplot as plt

# Returns a list of nodes with highest degrees
def highest_degree_nodes(graph, testing_graph, count, max_degree, directed):
    
    degrees = sorted(list(graph.in_degree()), key = lambda x : x[1], reverse = True) if directed \
                          else sorted(list(graph.degree()), key = lambda x : x[1], reverse = True)
    nodes = []
    for i in degrees:
        if count and i[1] <= max_degree and testing_graph.has_node(i[0]):
            nodes.append(i[0])
            count -= 1
    return nodes

# Loads the dataset and returns a list of strong attractors after removing saturated nodes
def load_and_preprocess(network, training_graph, testing_graph):
    
    # print("Generating training and testing graph...")
    dataset = open(network.filepath, 'r')  # reading data from dataset file
    for edge in dataset:
        values = edge.split(',') if network.comma_separated else edge.split()
        src, dest = int(values[network.src_col]), int(values[network.dest_col])
        if src == dest: continue
        probability = rn.random()
        if probability >= .1:
            training_graph.add_edge(src, dest)  # training graph
        else:
            testing_graph.add_edge(src, dest)  # testing graph
    # print("Training and testing graph generated!!")
    
    # print('Nodes in {} dataset prediction graph:'.format(network.name), len(training_graph.nodes()))
    # print('Nodes in {} dataset verification graph:'.format(network.name), len(testing_graph.nodes()))
    # print('Edges in {} dataset prediction graph:'.format(network.name), len(training_graph.edges()))
    # print('Edges in {} dataset verification graph:'.format(network.name), len(testing_graph.edges()))
    
    # print("Identifying strong attractors...")
    strong_attractors = highest_degree_nodes(training_graph, testing_graph, network.str_attrs, 
                                             network.max_degree, network.is_directed)
    # print("Strong attractors identified!!")
    
    return strong_attractors

# Predict the state of network for a strong attractor after propagating information
def predict_links(attr, graph, network):
    
    round_limit, prop_threshold, directed = network.round_limit, network.prop_threshold, network.is_directed
    
    state = {attr: [prop_threshold, 'recvd']}  # {node : [info_recvd, curr_state(received/active/propagated)]}
    active_nodes = [attr]  # stores newly activated nodes for each round
    round_no = 0
    
    while active_nodes and round_no <= round_limit:
        DIRECT_INFO = 1 / 2 ** round_no
        INDIRECT_INFO = 1 / 2 ** (round_no + 1)
        round_no += 1
        new_actives = []
        
        for node in active_nodes:
            state[node][1] = 'propgtd'
                            
            # Propagating information to neighbours of active node
            neighbours = list(graph.predecessors(node)) if directed else list(graph.adj[node].keys())
            
            for neigh in neighbours:
                if neigh in state:
                    state[neigh][0] += DIRECT_INFO
                    if state[neigh][0] >= prop_threshold and state[neigh][1] == 'recvd':
                        new_actives.append(neigh)
                        state[neigh][1] = 'active'
                else:
                    state[neigh] = [DIRECT_INFO, 'recvd']
                
                # Propagating information to distance neighbours of active node
                if round_no <= round_limit:
                    dist_neighbours = list(graph.predecessors(neigh)) if directed else list(graph.adj[neigh].keys())
                    
                    for dist_neigh in dist_neighbours:
                        if dist_neigh == node:
                            continue
                        if dist_neigh in state:
                            state[dist_neigh][0] += INDIRECT_INFO
                            if state[dist_neigh][0] >= prop_threshold and state[dist_neigh][1] == 'recvd':
                                new_actives.append(dist_neigh)
                                state[dist_neigh][1] = 'active'
                        else:
                            state[dist_neigh] = [INDIRECT_INFO, 'recvd']
        
        active_nodes = new_actives
        
    return state

# Implementation of adamic adar, preferential attachment and jaccard coefficient to compare results of undirected networks
def standard_techniques(attr, training_graph, testing_graph):
    edge_set = []
    for node in training_graph.nodes():
        if node == attr: continue
        edge_set.append((node, attr))
    
    adamic_index = nx.adamic_adar_index(training_graph, edge_set)
    pref_index = nx.preferential_attachment(training_graph, edge_set)
    jacc_coeff = nx.jaccard_coefficient(training_graph, edge_set)
    return adamic_index, pref_index, jacc_coeff

# Provides count of greater and equal values for other indexes
def standard_techniques_params(training_graph, testing_graph, index_iter):
    correct_links, incorrect_links = [], []
    for node, attr, index in index_iter:
        if training_graph.has_edge(node, attr):
            continue
        elif testing_graph.has_edge(node, attr):
            correct_links.append(round(index, 3))
        else:
            incorrect_links.append(round(index, 3))
    
    correct_links.sort()
    incorrect_links.sort()

    greater_index, equal_index = 0, 0
    for value in correct_links:
        count = bisect.bisect_left(incorrect_links, value)
        greater_index += count
        equal_index += bisect.bisect_right(incorrect_links, value) - count
    
    return greater_index, equal_index

# Returns a list of AUC parameters for a strong attractor
def calculate_auc_params(network, training_graph, testing_graph, attr):
    
    # Get state of network after information diffusion
    state = predict_links(attr, training_graph, network)
    correct_links, incorrect_links = [], []
    
    for node in list(state.keys()):
        if attr == node or training_graph.has_edge(node, attr):
            continue
        elif testing_graph.has_edge(node, attr):
            correct_links.append(round(state[node][0], 3))
        else:
            incorrect_links.append(round(state[node][0], 3))
    
    correct_links.sort()
    incorrect_links.sort()
    equal, greater = 0, 0
    
    for value in correct_links:
        count = bisect.bisect_left(incorrect_links, value)
        greater += count
        equal += bisect.bisect_right(incorrect_links, value) - count

    all_possible_links = network.nodes - 1 - ((training_graph.in_degree[attr] - \
                            testing_graph.in_degree[attr]) if network.is_directed \
                            else (training_graph.degree[attr] + testing_graph.degree[attr]))

    count = 0
    for node in list(testing_graph.adj[attr].keys()):
        if training_graph.has_node(node):
            count += 1
    
    # all_possible_links = training_graph.number_of_nodes() - 1 - ((training_graph.in_degree[attr] - \
    #                         testing_graph.in_degree[attr]) if network.is_directed \
    #                         else (training_graph.degree[attr] + count))

    greater += len(correct_links) * (all_possible_links - len(incorrect_links))
    
    equal += ((testing_graph.in_degree[attr] if network.is_directed else \
                            testing_graph.degree[attr]) - len(correct_links)) \
                            * (all_possible_links - len(incorrect_links))
    
    # equal = (count - len(correct_links)) * (all_possible_links - len(incorrect_links))

    total_combinations = all_possible_links * (testing_graph.in_degree[attr] \
                            if network.is_directed else testing_graph.degree[attr])
    
    # total_combinations = all_possible_links * count
    
    # Other indexes cannot be calculated for directed networks
    if network.is_directed:
        return [greater, equal, total_combinations] + [0] * 6
    # return [greater, equal, total_combinations, 0, 0]

    # Calculating adamic adar index, preferential attachement and jaccard coefficient for undirected networks
    adamic_index, pref_index, jacc_coeff = standard_techniques(attr, training_graph, testing_graph)
    
    params = [greater, equal, total_combinations]
    params += list(standard_techniques_params(training_graph, testing_graph, adamic_index))
    params += list(standard_techniques_params(training_graph, testing_graph, pref_index))
    params += list(standard_techniques_params(training_graph, testing_graph, jacc_coeff))
    
    return params

# Calculate and returns the AUC value for a network by combining results from all strong attractors
def calculate_auc(network, training_graph, testing_graph, strong_attractors):
    
    print("Calculating AUCs for {} dataset...".format(network.name))
    calc_auc_params = partial(calculate_auc_params, network, training_graph, testing_graph)
    
    with Pool(processes=3) as p:
        result = p.map(calc_auc_params, strong_attractors)
    
    greater, equal, total = 0, 0, 0
    greater_adamic, equal_adamic = 0, 0
    greater_pref, equal_pref = 0, 0
    greater_jacc, equal_jacc = 0, 0

    auc_info_diffusion, auc_adamic_adar, auc_pref_index, auc_jacc_coeff = [], [], [], []
    counter = 0
    
    for row in result:
        counter += 1
        greater += row[0]
        equal += row[1]
        total += row[2]
        greater_adamic += row[3]
        equal_adamic += row[4]
        greater_pref += row[5]
        equal_pref += row[6]
        greater_jacc += row[7]
        equal_jacc += row[8]
        if counter in list(range(10, 101, 10)):
            auc_info_diffusion.append((greater + 0.5 * equal) / total)
            auc_adamic_adar.append((greater_adamic + 0.5 * equal_adamic) / total)
            auc_pref_index.append((greater_pref + 0.5 * equal_pref) / total)
            auc_jacc_coeff.append((greater_jacc + 0.5 * equal_jacc) / total)

    return auc_info_diffusion, auc_adamic_adar, auc_pref_index, auc_jacc_coeff

# Produce curves plotting AUC values for different algorithms
def plot_auc_curve(name, results, xvalues):
    markers = ['*', 'o', 'D', '^', 'p', 's', 'h', '1']
    colors = ['blue', 'green', 'yellow', 'magenta', 'red', 'cyan']

    plt.plot(xvalues, results[0], marker=markers[0], markerfacecolor=colors[0], 
             color=colors[0], markersize=12, label='Information diffiusion')
    
    plt.plot(xvalues, results[1], marker=markers[1], markerfacecolor=colors[1], 
             color=colors[1], markersize=12, label='Adamic adar')
    
    # plt.plot(xvalues, results[2], marker=markers[2], markerfacecolor=colors[2], 
    #          color=colors[2], markersize=12, label='Preferential Attachment')
    
    plt.plot(xvalues, results[3], marker=markers[3], markerfacecolor=colors[3], 
             color=colors[3], markersize=12, label='Jaccard coefficient')
    
    plt.xlabel('Number of strong attractors under consideration')
    plt.ylabel('AUC values')
    plt.title('AUC variation for {} dataset'.format(name))
    plt.legend()
    plt.show()

# Driver method for link prediction in social network
def network_main(network):
    
    if network.is_directed:
        training_graph = nx.DiGraph()
        testing_graph = nx.DiGraph()
    else:
        training_graph = nx.Graph()
        testing_graph = nx.Graph()
    strong_attractors = load_and_preprocess(network, training_graph, testing_graph)
    auc_info_diffusion, auc_adamic_adar, auc_pref_index, auc_jacc_coeff = calculate_auc(network, training_graph, testing_graph, strong_attractors)
    print("AUCs by Information Diffusion for {} dataset is: {}".format(network.name, auc_info_diffusion))
    print("AUCs by Adamic Adar Index for {} dataset is: {}".format(network.name, auc_adamic_adar))
    print("AUCs by Preferential Attachment for {} dataset is: {}".format(network.name, auc_pref_index))
    print("AUCs by Jaccard Coefficient for {} dataset is: {}".format(network.name, auc_jacc_coeff))

    plot_auc_curve(network.name, [auc_info_diffusion, auc_adamic_adar, auc_pref_index, auc_jacc_coeff], list(range(10, 101, 10)))

# Defines properties of a social network
class SocialNetwork:
    
    def __init__(self, name, filepath, nodes = 0, 
                    edges = "unspecified", is_directed = False, 
                    src_col = 0, dest_col = 1, comma_separated = False, 
                    max_degree = 100, str_attrs = 100, prop_threshold = 20, 
                    round_limit = 2, edge_limit = 0.1
                ):
        
        self.name = name
        self.filepath = filepath
        self.nodes = nodes
        self.edges = edges
        self.is_directed = is_directed
        self.src_col = src_col
        self.dest_col = dest_col
        self.comma_separated = comma_separated
        self.max_degree = max_degree
        self.str_attrs = str_attrs
        self.prop_threshold = prop_threshold
        self.round_limit = round_limit
        self.edge_limit = edge_limit

# Results cannot be compared with adamic adar though AUC value is good
twitter = SocialNetwork("Twitter", "/content/drive/My Drive/network_datasets/graph_cb.txt", 
                        nodes=90909, is_directed=True, str_attrs=100, prop_threshold=20, round_limit=2)
# Almost the same result but second time our algorithm was better
hepth = SocialNetwork(name="HEP-TH", filepath="/home/aryan/4th Year Project/HEP-TH/Cit-HepTh.txt", 
                      nodes=27770, is_directed=False, str_attrs=100, prop_threshold=15, round_limit=2)
# Our algorithm performs better but results are bad for undirected
wiki_vote = SocialNetwork(name="Wiki-Vote", filepath="/content/drive/My Drive/network_datasets/Wiki-Vote.txt", 
                          nodes=7115, is_directed=True, str_attrs=100, prop_threshold=20, round_limit=2)
# Great results but comparable
facebook_small = SocialNetwork(name="Facebook Small", filepath="/home/aryan/4th Year Project/small facebook/facebook_combined.txt", 
                               nodes=4039, str_attrs=100, prop_threshold=15, round_limit=2)
# Too large to process
citeseer = SocialNetwork(name="Citeseer", filepath="/content/drive/My Drive/network_datasets/out.citeseer", 
                         nodes=384413, str_attrs=100, is_directed=True, prop_threshold=20, round_limit=2)
# Results are too bad
cora_citation = SocialNetwork(name="Cora Citation", filepath="/content/drive/My Drive/network_datasets/out.subelj_cora_cora", 
                              nodes=23166, str_attrs=100, is_directed=True, prop_threshold=20, round_limit=2)
slashdot = SocialNetwork(name="Slashdot", filepath="/content/drive/My Drive/network_datasets/Slashdot0902.txt", 
                         nodes=82168, str_attrs=100, is_directed=True, prop_threshold=20, round_limit=2)
# Gowalla gives good results
gowalla = SocialNetwork(name="Gowalla", filepath="/home/aryan/4th Year Project/Gowalla/out.loc-gowalla_edges", 
                        nodes=196591, str_attrs=100, prop_threshold=30, round_limit=2)
# Results are almost similar
petster = SocialNetwork(name="Petster Hamster", filepath="/home/aryan/4th Year Project/petster-hamster/out.petster-hamster", 
                        nodes=2426, str_attrs=100, prop_threshold=8, round_limit=2)
# Results are better for our algorithm
brightkite = SocialNetwork(name="Brightkite", filepath="/home/aryan/4th Year Project/loc-brightkite_edges/out.loc-brightkite_edges", 
                           nodes=58228, str_attrs=100, prop_threshold=8, round_limit=2)
# Provides better results for information diffusion
facebook_large = SocialNetwork(name="Facebook Large", filepath="/home/aryan/4th Year Project/facebook-wosn-links/out.facebook-wosn-links", 
                               nodes=63731, str_attrs=100, prop_threshold=20, round_limit=2)
# Did not provide good results
twitch = SocialNetwork(name="Twitch", filepath="/content/drive/My Drive/network_datasets/musae_DE_edges.csv", 
                       nodes=9498, str_attrs=100, prop_threshold=20, round_limit=2, comma_separated=True)
# Too large dataset
digg = SocialNetwork(name="Digg Friends", filepath="/content/drive/My Drive/network_datasets/out.digg-friends", 
                     nodes=279630, is_directed=True, prop_threshold=20)
# Not a suitable dataset
epinions = SocialNetwork(name="Epinions", filepath="/content/drive/My Drive/network_datasets/out.epinions", 
                        nodes=131828, is_directed=True, prop_threshold=30)
# The dataset is too large
dogster = SocialNetwork(name="Dogster", filepath="/content/drive/My Drive/network_datasets/out.petster-friendships-dog-uniq", 
                        nodes=426820, is_directed=False, prop_threshold=20)
# Results aren't so good
livemocha = SocialNetwork(name="Livemocha", filepath="/content/drive/My Drive/network_datasets/out.livemocha", 
                        nodes=104103, is_directed=False, prop_threshold=20)
# Results were better for our algorithm even for 1000 strong attractors
hepph = SocialNetwork(name="HEP-PH", filepath="/home/aryan/4th Year Project/cit-HepPh/out.cit-HepPh", 
                      nodes=34546, is_directed=False, str_attrs=100, prop_threshold=20, round_limit=2)

datasets_part1 = {
    'Facebook_large': facebook_large,
    'Facebook_small': facebook_small,
    'Hepth': hepth,
    'Hepph': hepph,
    'Brightkite': brightkite,
    'Gowalla': gowalla,
    'Petster': petster,
}

datasets_part1 = {
    'livemocha': livemocha,
    'dogster': dogster,
    'digg': digg,
    'slashdot': slashdot,
    'twitter': twitter,
    'wiki_vote': wiki_vote,
    'citeseer': citeseer,
}

datasets_part1 = {
    # 'Petster': petster,
    'Facebook_large': gowalla,
    # 'Facebook_small': facebook_small,
}

processes = []
for dataset in datasets_part1:
    datasets_part1[dataset].str_attrs = 100
    datasets_part1[dataset].prop_threshold = 20
    network_main(datasets_part1[dataset])
    # datasets_part1[dataset].is_directed = False
    # p = Process(target=network_main, args=(datasets_part1[dataset],))
    # p.start()
    # print("Started new process")
    # processes.append(p)
# for process in processes:
#     process.join()
